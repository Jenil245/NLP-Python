{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JZhsGRg92sC"
   },
   "source": [
    "<b>Creating model which will generate text about particular topic\n",
    "</b><br>\n",
    "Here, Data will be collected from wikipedia website, You just need to give name of particular topic as per wikipedia page.<br>\n",
    " You can also provide multiple topic as list but I wouldn't recommend that unless you can train model with more epochs\n",
    "\n",
    "As an example I have taken Deep Learning as particular topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rb00FpA692sG"
   },
   "source": [
    "# DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UepOA-uM92sI"
   },
   "source": [
    "Data can be acquired by performing web scrapping on wikipedia web pages <br>\n",
    "Here, I will use beautiful soup and requests module for web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIO_lfLE92sJ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELXBb1wx92sN"
   },
   "outputs": [],
   "source": [
    "def get_data(topics):\n",
    "    text_data=[]\n",
    "   \n",
    "    for topic in topics:\n",
    "        wiki_pages=requests.get('https://en.wikipedia.org/wiki/'+topic)\n",
    "        soup=BeautifulSoup(wiki_pages.text,'lxml')\n",
    "        find_p=soup.find(\"div\",{\"class\":\"mw-content-ltr\"}).find_all(\"p\")\n",
    "      \n",
    "        for p in find_p:\n",
    "            if len(p.text)>10:\n",
    "                text_data.append(p.text)\n",
    "        \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19qx36jn92sQ"
   },
   "outputs": [],
   "source": [
    "data=get_data(['Deep_learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "wRd8sU_f92sU",
    "outputId": "42c7a2e7-61ce-4a23-9e22-9a406af543bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 \n",
      "\n",
      "For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(data),'\\n')\n",
    "print(data[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClSaohuFcchO"
   },
   "source": [
    "# DATA PROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lEM-9OKY92sa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiLTbSl4HuRH"
   },
   "source": [
    "By performing keras tokenizer on data, We can get meaningful insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "YHBUhmrV92se",
    "outputId": "935c9b6b-d9e6-4602-9059-e03657e4dced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 \n",
      "\n",
      "Found 2059 unique tokens. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(data)\n",
    "    \n",
    "print(tokenizer.document_count,'\\n')   #Number of documents (texts/sequences) the tokenizer was trained on\n",
    "    \n",
    "print('Found {0} unique tokens.'.format(len(tokenizer.word_index)),'\\n')    #total number of unique words\n",
    "    \n",
    "#print(tokenizer.word_index,'\\n')       #dictionary mapping words to their rank/index (int)\n",
    "    \n",
    "#print(tokenizer.word_counts,'\\n')           #dictionary mapping words to the number of times they appeared on during fit\n",
    "\n",
    "#print(tokenizer.word_docs)             #dictionary mapping words to the number of documents/texts they appeared on during fit        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7zYFqzjIuV-"
   },
   "source": [
    "Here I have defined two prepare data function<br>\n",
    "One will create n-gram predictors no matter the size and other one will create 4-grams predictors\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwtqF28qsZKy"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    tz = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "    tz.fit_on_texts(data)\n",
    "    total_words = len(tz.word_index)+1\n",
    "    \n",
    "    input_sequences = []\n",
    "    for line in data:\n",
    "        token_list = tz.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)    \n",
    "    max_len_sequence = max([len(x) for x in input_sequences])\n",
    "    padded_sequence = np.array(pad_sequences(input_sequences, maxlen = max_len_sequence, padding = 'pre'))  #using pad_sequence for generating same dimensions training data\n",
    "    predictors, label = padded_sequence[:,:-1],padded_sequence[:,-1]            #splliting predictors and target variable\n",
    "    label = ku.to_categorical(label, num_classes = total_words)   \n",
    "    \n",
    "    return predictors,label,max_len_sequence,total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxoHUM2LfCUi"
   },
   "outputs": [],
   "source": [
    "def prepare_data_4(data):\n",
    "    tz = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "    tz.fit_on_texts(data)\n",
    "    total_words = len(tz.word_index)+1\n",
    "    \n",
    "    input_sequences = []\n",
    "    for line in data:\n",
    "        count=0\n",
    "        token_list = tz.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            if i<=4:\n",
    "                n_gram_sequence = token_list[:i+1]\n",
    "            else:\n",
    "                count+=1\n",
    "                n_gram_sequence = token_list[count:i+1]\n",
    "            input_sequences.append(n_gram_sequence)    \n",
    "    max_len_sequence = max([len(x) for x in input_sequences])\n",
    "    padded_sequence = np.array(pad_sequences(input_sequences, maxlen = max_len_sequence, padding = 'pre'))\n",
    "    predictors, label = padded_sequence[:,:-1],padded_sequence[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes = total_words)   \n",
    "    \n",
    "    return predictors,label,max_len_sequence,total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "761t8TdFcdOB"
   },
   "outputs": [],
   "source": [
    "predictors,labels,max_len_sequence,total_words = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "qAXuGIHt92so",
    "outputId": "eea59d10-e05a-488e-982c-21445cd1b010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6941, 2060) \n",
      "\n",
      "(6941, 298) \n",
      "\n",
      "299 \n",
      "\n",
      "2060\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape,'\\n')\n",
    "print(predictors.shape,'\\n')\n",
    "print(max_len_sequence,'\\n')\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUVI61J2Ey1k"
   },
   "outputs": [],
   "source": [
    "predictors_4,labels_4,max_len_sequence_4,total_words_4 = prepare_data_4(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "XY3BnPYoFcYU",
    "outputId": "025db073-449b-4cca-e981-258c80b3304f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6941, 2060) \n",
      "\n",
      "(6941, 4) \n",
      "\n",
      "5 \n",
      "\n",
      "2060\n"
     ]
    }
   ],
   "source": [
    "print(labels_4.shape,'\\n')\n",
    "print(predictors_4.shape,'\\n')\n",
    "print(max_len_sequence_4,'\\n')\n",
    "print(total_words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "zkmrRmSeLR5A",
    "outputId": "3894ceb7-04d5-4f51-8acc-3d1324e16ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0   7]\n",
      " [  0   0   0 ...   0   7   8]\n",
      " [  0   0   0 ...   7   8  58]\n",
      " [  0   0   0 ...   8  58 490]\n",
      " [  0   0   0 ...  58 490  10]] \n",
      "\n",
      "[[  0   0   0   7]\n",
      " [  0   0   7   8]\n",
      " [  0   7   8  58]\n",
      " [  7   8  58 490]\n",
      " [  8  58 490  10]]\n"
     ]
    }
   ],
   "source": [
    "print(predictors[:5],'\\n')\n",
    "print(predictors_4[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoFa_dyxL4Vd"
   },
   "source": [
    "In n-gram predictors, label will be classified by considering n previous words where as in 4-gram predictors, label will be classified by just four previous words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwTOny8jdCfH"
   },
   "source": [
    "# MODEL DEVELOPEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsztbEdaNAyD"
   },
   "source": [
    "I will use LSTM layer with 100 units since data is less<br>\n",
    "\n",
    "If data is huge then GRU works better in terms of time and GPU and output is nearly same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0t0CRwNLAsk"
   },
   "outputs": [],
   "source": [
    "def create_model(max_len_sequence, total_words):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(total_words, 10, input_length=max_len_sequence - 1))\n",
    "    \n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    \n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "xSHCv94y92ss",
    "outputId": "5c38110d-e387-4461-ea9a-f1f20b9d4e39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 298, 10)           20600     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2060)              208060    \n",
      "=================================================================\n",
      "Total params: 273,060\n",
      "Trainable params: 273,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_len_sequence, total_words)\n",
    "\n",
    "print(model.summary(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "QY4_zWTxGs7X",
    "outputId": "e6212d3f-6366-48f1-fcb4-fa957fc6d441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 10)             20600     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2060)              208060    \n",
      "=================================================================\n",
      "Total params: 273,060\n",
      "Trainable params: 273,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_4 = create_model(max_len_sequence_4,total_words_4)\n",
    "\n",
    "print(model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G8gEDoLp92sw",
    "outputId": "b3070480-faa7-4e5e-dc07-495224285b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6941/6941 [==============================] - 103s 15ms/step - loss: 6.8386 - acc: 0.0434\n",
      "Epoch 2/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 6.5130 - acc: 0.0435\n",
      "Epoch 3/100\n",
      "6941/6941 [==============================] - 97s 14ms/step - loss: 6.4232 - acc: 0.0454\n",
      "Epoch 4/100\n",
      "6941/6941 [==============================] - 96s 14ms/step - loss: 6.3376 - acc: 0.0510\n",
      "Epoch 5/100\n",
      "6941/6941 [==============================] - 97s 14ms/step - loss: 6.2428 - acc: 0.0571\n",
      "Epoch 6/100\n",
      "6941/6941 [==============================] - 98s 14ms/step - loss: 6.1484 - acc: 0.0621\n",
      "Epoch 7/100\n",
      "6941/6941 [==============================] - 98s 14ms/step - loss: 6.0537 - acc: 0.0638\n",
      "Epoch 8/100\n",
      "6941/6941 [==============================] - 96s 14ms/step - loss: 5.9502 - acc: 0.0686\n",
      "Epoch 9/100\n",
      "6941/6941 [==============================] - 94s 14ms/step - loss: 5.8455 - acc: 0.0735\n",
      "Epoch 10/100\n",
      "6941/6941 [==============================] - 94s 14ms/step - loss: 5.7341 - acc: 0.0748\n",
      "Epoch 11/100\n",
      "6941/6941 [==============================] - 94s 14ms/step - loss: 5.6327 - acc: 0.0777\n",
      "Epoch 12/100\n",
      "6941/6941 [==============================] - 96s 14ms/step - loss: 5.5281 - acc: 0.0802\n",
      "Epoch 13/100\n",
      "6941/6941 [==============================] - 97s 14ms/step - loss: 5.4216 - acc: 0.0834\n",
      "Epoch 14/100\n",
      "6941/6941 [==============================] - 96s 14ms/step - loss: 5.3190 - acc: 0.0875\n",
      "Epoch 15/100\n",
      "6941/6941 [==============================] - 95s 14ms/step - loss: 5.2154 - acc: 0.0876\n",
      "Epoch 16/100\n",
      "6941/6941 [==============================] - 94s 14ms/step - loss: 5.1097 - acc: 0.0908\n",
      "Epoch 17/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 4.9987 - acc: 0.0968\n",
      "Epoch 18/100\n",
      "6941/6941 [==============================] - 95s 14ms/step - loss: 4.8972 - acc: 0.1016\n",
      "Epoch 19/100\n",
      "6941/6941 [==============================] - 94s 14ms/step - loss: 4.7904 - acc: 0.1059\n",
      "Epoch 20/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 4.6883 - acc: 0.1092\n",
      "Epoch 21/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 4.5796 - acc: 0.1158\n",
      "Epoch 22/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 4.4778 - acc: 0.1271\n",
      "Epoch 23/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 4.3740 - acc: 0.1380\n",
      "Epoch 24/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 4.2673 - acc: 0.1531\n",
      "Epoch 25/100\n",
      "6941/6941 [==============================] - 95s 14ms/step - loss: 4.1726 - acc: 0.1645\n",
      "Epoch 26/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 4.0755 - acc: 0.1792\n",
      "Epoch 27/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 3.9831 - acc: 0.2024\n",
      "Epoch 28/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 3.8805 - acc: 0.2217\n",
      "Epoch 29/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 3.7847 - acc: 0.2419\n",
      "Epoch 30/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 3.6884 - acc: 0.2566\n",
      "Epoch 31/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 3.6026 - acc: 0.2794\n",
      "Epoch 32/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 3.5152 - acc: 0.2975\n",
      "Epoch 33/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 3.4214 - acc: 0.3106\n",
      "Epoch 34/100\n",
      "6941/6941 [==============================] - 97s 14ms/step - loss: 3.3455 - acc: 0.3259\n",
      "Epoch 35/100\n",
      "6941/6941 [==============================] - 98s 14ms/step - loss: 3.2563 - acc: 0.3420\n",
      "Epoch 36/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 3.1779 - acc: 0.3551\n",
      "Epoch 37/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 3.0998 - acc: 0.3685\n",
      "Epoch 38/100\n",
      "6941/6941 [==============================] - 100s 14ms/step - loss: 3.0243 - acc: 0.3785\n",
      "Epoch 39/100\n",
      "6941/6941 [==============================] - 100s 14ms/step - loss: 2.9481 - acc: 0.3936\n",
      "Epoch 40/100\n",
      "6941/6941 [==============================] - 100s 14ms/step - loss: 2.8917 - acc: 0.4047\n",
      "Epoch 41/100\n",
      "6941/6941 [==============================] - 101s 15ms/step - loss: 2.8254 - acc: 0.4113\n",
      "Epoch 42/100\n",
      "6941/6941 [==============================] - 102s 15ms/step - loss: 2.7512 - acc: 0.4259\n",
      "Epoch 43/100\n",
      "6941/6941 [==============================] - 103s 15ms/step - loss: 2.6999 - acc: 0.4365\n",
      "Epoch 44/100\n",
      "6941/6941 [==============================] - 102s 15ms/step - loss: 2.6452 - acc: 0.4413\n",
      "Epoch 45/100\n",
      "6941/6941 [==============================] - 100s 14ms/step - loss: 2.5847 - acc: 0.4556\n",
      "Epoch 46/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.5281 - acc: 0.4623\n",
      "Epoch 47/100\n",
      "6941/6941 [==============================] - 100s 14ms/step - loss: 2.4791 - acc: 0.4775\n",
      "Epoch 48/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.4226 - acc: 0.4835\n",
      "Epoch 49/100\n",
      "6941/6941 [==============================] - 97s 14ms/step - loss: 2.3822 - acc: 0.4910\n",
      "Epoch 50/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.3313 - acc: 0.5002\n",
      "Epoch 51/100\n",
      "6941/6941 [==============================] - 98s 14ms/step - loss: 2.2841 - acc: 0.5106\n",
      "Epoch 52/100\n",
      "6941/6941 [==============================] - 98s 14ms/step - loss: 2.2289 - acc: 0.5171\n",
      "Epoch 53/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.1920 - acc: 0.5285\n",
      "Epoch 54/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.1603 - acc: 0.5381\n",
      "Epoch 55/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.1123 - acc: 0.5417\n",
      "Epoch 56/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.0783 - acc: 0.5475\n",
      "Epoch 57/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.0444 - acc: 0.5565\n",
      "Epoch 58/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 2.0123 - acc: 0.5587\n",
      "Epoch 59/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 1.9741 - acc: 0.5695\n",
      "Epoch 60/100\n",
      "6941/6941 [==============================] - 100s 14ms/step - loss: 1.9252 - acc: 0.5763\n",
      "Epoch 61/100\n",
      "6941/6941 [==============================] - 99s 14ms/step - loss: 1.8997 - acc: 0.5800\n",
      "Epoch 62/100\n",
      "6941/6941 [==============================] - 97s 14ms/step - loss: 1.8706 - acc: 0.5855\n",
      "Epoch 63/100\n",
      "6941/6941 [==============================] - 96s 14ms/step - loss: 1.8338 - acc: 0.5980\n",
      "Epoch 64/100\n",
      "6941/6941 [==============================] - 95s 14ms/step - loss: 1.8055 - acc: 0.6045\n",
      "Epoch 65/100\n",
      "6941/6941 [==============================] - 94s 14ms/step - loss: 1.7728 - acc: 0.6090\n",
      "Epoch 66/100\n",
      "6941/6941 [==============================] - 96s 14ms/step - loss: 1.7428 - acc: 0.6094\n",
      "Epoch 67/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.7142 - acc: 0.6152\n",
      "Epoch 68/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.6864 - acc: 0.6240\n",
      "Epoch 69/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.6548 - acc: 0.6342\n",
      "Epoch 70/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.6301 - acc: 0.6364\n",
      "Epoch 71/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.6072 - acc: 0.6411\n",
      "Epoch 72/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.5828 - acc: 0.6479\n",
      "Epoch 73/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.5550 - acc: 0.6541\n",
      "Epoch 74/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.5234 - acc: 0.6600\n",
      "Epoch 75/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.5089 - acc: 0.6547\n",
      "Epoch 76/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.4900 - acc: 0.6672\n",
      "Epoch 77/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.4643 - acc: 0.6712\n",
      "Epoch 78/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.4634 - acc: 0.6671\n",
      "Epoch 79/100\n",
      "6941/6941 [==============================] - 96s 14ms/step - loss: 1.4203 - acc: 0.6804\n",
      "Epoch 80/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.4086 - acc: 0.6799\n",
      "Epoch 81/100\n",
      "6941/6941 [==============================] - 97s 14ms/step - loss: 1.3761 - acc: 0.6829\n",
      "Epoch 82/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.3589 - acc: 0.6915\n",
      "Epoch 83/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.3486 - acc: 0.6936\n",
      "Epoch 84/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.3229 - acc: 0.6960\n",
      "Epoch 85/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.2953 - acc: 0.7047\n",
      "Epoch 86/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.2823 - acc: 0.7091\n",
      "Epoch 87/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.2635 - acc: 0.7116\n",
      "Epoch 88/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.2365 - acc: 0.7160\n",
      "Epoch 89/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.2256 - acc: 0.7179\n",
      "Epoch 90/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.2132 - acc: 0.7202\n",
      "Epoch 91/100\n",
      "6941/6941 [==============================] - 94s 14ms/step - loss: 1.1992 - acc: 0.7221\n",
      "Epoch 92/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.1694 - acc: 0.7398\n",
      "Epoch 93/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.1760 - acc: 0.7260\n",
      "Epoch 94/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.1492 - acc: 0.7387\n",
      "Epoch 95/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.1254 - acc: 0.7366\n",
      "Epoch 96/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.1102 - acc: 0.7472\n",
      "Epoch 97/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.0924 - acc: 0.7503\n",
      "Epoch 98/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.0854 - acc: 0.7525\n",
      "Epoch 99/100\n",
      "6941/6941 [==============================] - 92s 13ms/step - loss: 1.0595 - acc: 0.7525\n",
      "Epoch 100/100\n",
      "6941/6941 [==============================] - 93s 13ms/step - loss: 1.0557 - acc: 0.7582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0325ac8748>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YwLE9y4DHPPO",
    "outputId": "9db7649b-ff4e-4e05-dd01-0fe4f84a45d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6941/6941 [==============================] - 3s 462us/step - loss: 7.0161 - acc: 0.0419\n",
      "Epoch 2/100\n",
      "6941/6941 [==============================] - 2s 351us/step - loss: 6.5344 - acc: 0.0435\n",
      "Epoch 3/100\n",
      "6941/6941 [==============================] - 3s 362us/step - loss: 6.4599 - acc: 0.0435\n",
      "Epoch 4/100\n",
      "6941/6941 [==============================] - 2s 354us/step - loss: 6.4094 - acc: 0.0431\n",
      "Epoch 5/100\n",
      "6941/6941 [==============================] - 2s 360us/step - loss: 6.3643 - acc: 0.0435\n",
      "Epoch 6/100\n",
      "6941/6941 [==============================] - 2s 351us/step - loss: 6.3127 - acc: 0.0435\n",
      "Epoch 7/100\n",
      "6941/6941 [==============================] - 2s 342us/step - loss: 6.2571 - acc: 0.0435\n",
      "Epoch 8/100\n",
      "6941/6941 [==============================] - 3s 361us/step - loss: 6.1979 - acc: 0.0439\n",
      "Epoch 9/100\n",
      "6941/6941 [==============================] - 2s 346us/step - loss: 6.1416 - acc: 0.0457\n",
      "Epoch 10/100\n",
      "6941/6941 [==============================] - 2s 343us/step - loss: 6.0796 - acc: 0.0510\n",
      "Epoch 11/100\n",
      "6941/6941 [==============================] - 2s 360us/step - loss: 6.0117 - acc: 0.0545\n",
      "Epoch 12/100\n",
      "6941/6941 [==============================] - 2s 350us/step - loss: 5.9325 - acc: 0.0607\n",
      "Epoch 13/100\n",
      "6941/6941 [==============================] - 2s 347us/step - loss: 5.8412 - acc: 0.0673\n",
      "Epoch 14/100\n",
      "6941/6941 [==============================] - 2s 353us/step - loss: 5.7511 - acc: 0.0712\n",
      "Epoch 15/100\n",
      "6941/6941 [==============================] - 2s 346us/step - loss: 5.6551 - acc: 0.0736\n",
      "Epoch 16/100\n",
      "6941/6941 [==============================] - 2s 341us/step - loss: 5.5609 - acc: 0.0795\n",
      "Epoch 17/100\n",
      "6941/6941 [==============================] - 2s 344us/step - loss: 5.4623 - acc: 0.0815\n",
      "Epoch 18/100\n",
      "6941/6941 [==============================] - 2s 344us/step - loss: 5.3661 - acc: 0.0864\n",
      "Epoch 19/100\n",
      "6941/6941 [==============================] - 2s 344us/step - loss: 5.2692 - acc: 0.0863\n",
      "Epoch 20/100\n",
      "6941/6941 [==============================] - 2s 343us/step - loss: 5.1620 - acc: 0.0921\n",
      "Epoch 21/100\n",
      "6941/6941 [==============================] - 2s 354us/step - loss: 5.0655 - acc: 0.0936\n",
      "Epoch 22/100\n",
      "6941/6941 [==============================] - 2s 345us/step - loss: 4.9679 - acc: 0.1003\n",
      "Epoch 23/100\n",
      "6941/6941 [==============================] - 2s 345us/step - loss: 4.8657 - acc: 0.1049\n",
      "Epoch 24/100\n",
      "6941/6941 [==============================] - 2s 344us/step - loss: 4.7666 - acc: 0.1124\n",
      "Epoch 25/100\n",
      "6941/6941 [==============================] - 2s 344us/step - loss: 4.6620 - acc: 0.1179\n",
      "Epoch 26/100\n",
      "6941/6941 [==============================] - 2s 348us/step - loss: 4.5592 - acc: 0.1264\n",
      "Epoch 27/100\n",
      "6941/6941 [==============================] - 2s 343us/step - loss: 4.4583 - acc: 0.1367\n",
      "Epoch 28/100\n",
      "6941/6941 [==============================] - 2s 354us/step - loss: 4.3661 - acc: 0.1421\n",
      "Epoch 29/100\n",
      "6941/6941 [==============================] - 2s 341us/step - loss: 4.2653 - acc: 0.1530\n",
      "Epoch 30/100\n",
      "6941/6941 [==============================] - 2s 342us/step - loss: 4.1681 - acc: 0.1644\n",
      "Epoch 31/100\n",
      "6941/6941 [==============================] - 2s 345us/step - loss: 4.0725 - acc: 0.1667\n",
      "Epoch 32/100\n",
      "6941/6941 [==============================] - 2s 344us/step - loss: 3.9871 - acc: 0.1850\n",
      "Epoch 33/100\n",
      "6941/6941 [==============================] - 2s 356us/step - loss: 3.8883 - acc: 0.2000\n",
      "Epoch 34/100\n",
      "6941/6941 [==============================] - 2s 342us/step - loss: 3.8038 - acc: 0.2139\n",
      "Epoch 35/100\n",
      "6941/6941 [==============================] - 2s 355us/step - loss: 3.7079 - acc: 0.2322\n",
      "Epoch 36/100\n",
      "6941/6941 [==============================] - 3s 371us/step - loss: 3.6181 - acc: 0.2478\n",
      "Epoch 37/100\n",
      "6941/6941 [==============================] - 3s 364us/step - loss: 3.5368 - acc: 0.2635\n",
      "Epoch 38/100\n",
      "6941/6941 [==============================] - 2s 357us/step - loss: 3.4483 - acc: 0.2770\n",
      "Epoch 39/100\n",
      "6941/6941 [==============================] - 2s 346us/step - loss: 3.3708 - acc: 0.2922\n",
      "Epoch 40/100\n",
      "6941/6941 [==============================] - 2s 344us/step - loss: 3.2847 - acc: 0.3098\n",
      "Epoch 41/100\n",
      "6941/6941 [==============================] - 2s 338us/step - loss: 3.1967 - acc: 0.3292\n",
      "Epoch 42/100\n",
      "6941/6941 [==============================] - 2s 333us/step - loss: 3.1223 - acc: 0.3455\n",
      "Epoch 43/100\n",
      "6941/6941 [==============================] - 2s 337us/step - loss: 3.0573 - acc: 0.3644\n",
      "Epoch 44/100\n",
      "6941/6941 [==============================] - 2s 330us/step - loss: 2.9784 - acc: 0.3706\n",
      "Epoch 45/100\n",
      "6941/6941 [==============================] - 2s 338us/step - loss: 2.8981 - acc: 0.3933\n",
      "Epoch 46/100\n",
      "6941/6941 [==============================] - 2s 356us/step - loss: 2.8251 - acc: 0.4061\n",
      "Epoch 47/100\n",
      "6941/6941 [==============================] - 2s 339us/step - loss: 2.7642 - acc: 0.4172\n",
      "Epoch 48/100\n",
      "6941/6941 [==============================] - 2s 338us/step - loss: 2.7051 - acc: 0.4341\n",
      "Epoch 49/100\n",
      "6941/6941 [==============================] - 2s 340us/step - loss: 2.6250 - acc: 0.4505\n",
      "Epoch 50/100\n",
      "6941/6941 [==============================] - 2s 336us/step - loss: 2.5659 - acc: 0.4592\n",
      "Epoch 51/100\n",
      "6941/6941 [==============================] - 2s 343us/step - loss: 2.5010 - acc: 0.4697\n",
      "Epoch 52/100\n",
      "6941/6941 [==============================] - 2s 337us/step - loss: 2.4350 - acc: 0.4858\n",
      "Epoch 53/100\n",
      "6941/6941 [==============================] - 2s 351us/step - loss: 2.3911 - acc: 0.4968\n",
      "Epoch 54/100\n",
      "6941/6941 [==============================] - 2s 339us/step - loss: 2.3244 - acc: 0.5076\n",
      "Epoch 55/100\n",
      "6941/6941 [==============================] - 2s 336us/step - loss: 2.2707 - acc: 0.5205\n",
      "Epoch 56/100\n",
      "6941/6941 [==============================] - 2s 334us/step - loss: 2.2204 - acc: 0.5292\n",
      "Epoch 57/100\n",
      "6941/6941 [==============================] - 2s 331us/step - loss: 2.1692 - acc: 0.5371\n",
      "Epoch 58/100\n",
      "6941/6941 [==============================] - 2s 354us/step - loss: 2.1259 - acc: 0.5470\n",
      "Epoch 59/100\n",
      "6941/6941 [==============================] - 2s 332us/step - loss: 2.0746 - acc: 0.5547\n",
      "Epoch 60/100\n",
      "6941/6941 [==============================] - 2s 339us/step - loss: 2.0294 - acc: 0.5679\n",
      "Epoch 61/100\n",
      "6941/6941 [==============================] - 2s 352us/step - loss: 1.9706 - acc: 0.5780\n",
      "Epoch 62/100\n",
      "6941/6941 [==============================] - 2s 347us/step - loss: 1.9293 - acc: 0.5810\n",
      "Epoch 63/100\n",
      "6941/6941 [==============================] - 2s 338us/step - loss: 1.8810 - acc: 0.5929\n",
      "Epoch 64/100\n",
      "6941/6941 [==============================] - 2s 342us/step - loss: 1.8448 - acc: 0.6015\n",
      "Epoch 65/100\n",
      "6941/6941 [==============================] - 2s 337us/step - loss: 1.7985 - acc: 0.6096\n",
      "Epoch 66/100\n",
      "6941/6941 [==============================] - 2s 337us/step - loss: 1.7662 - acc: 0.6156\n",
      "Epoch 67/100\n",
      "6941/6941 [==============================] - 2s 337us/step - loss: 1.7222 - acc: 0.6290\n",
      "Epoch 68/100\n",
      "6941/6941 [==============================] - 2s 332us/step - loss: 1.6854 - acc: 0.6342\n",
      "Epoch 69/100\n",
      "6941/6941 [==============================] - 2s 331us/step - loss: 1.6425 - acc: 0.6405\n",
      "Epoch 70/100\n",
      "6941/6941 [==============================] - 2s 331us/step - loss: 1.6073 - acc: 0.6526\n",
      "Epoch 71/100\n",
      "6941/6941 [==============================] - 2s 330us/step - loss: 1.5763 - acc: 0.6568\n",
      "Epoch 72/100\n",
      "6941/6941 [==============================] - 2s 352us/step - loss: 1.5299 - acc: 0.6702\n",
      "Epoch 73/100\n",
      "6941/6941 [==============================] - 2s 326us/step - loss: 1.4979 - acc: 0.6760\n",
      "Epoch 74/100\n",
      "6941/6941 [==============================] - 2s 327us/step - loss: 1.4650 - acc: 0.6812\n",
      "Epoch 75/100\n",
      "6941/6941 [==============================] - 2s 328us/step - loss: 1.4345 - acc: 0.6898\n",
      "Epoch 76/100\n",
      "6941/6941 [==============================] - 2s 339us/step - loss: 1.4043 - acc: 0.6949\n",
      "Epoch 77/100\n",
      "6941/6941 [==============================] - 2s 333us/step - loss: 1.3723 - acc: 0.7097\n",
      "Epoch 78/100\n",
      "6941/6941 [==============================] - 2s 340us/step - loss: 1.3336 - acc: 0.7052\n",
      "Epoch 79/100\n",
      "6941/6941 [==============================] - 2s 334us/step - loss: 1.3123 - acc: 0.7175\n",
      "Epoch 80/100\n",
      "6941/6941 [==============================] - 2s 333us/step - loss: 1.2809 - acc: 0.7208\n",
      "Epoch 81/100\n",
      "6941/6941 [==============================] - 2s 329us/step - loss: 1.2545 - acc: 0.7293\n",
      "Epoch 82/100\n",
      "6941/6941 [==============================] - 2s 330us/step - loss: 1.2359 - acc: 0.7270\n",
      "Epoch 83/100\n",
      "6941/6941 [==============================] - 2s 327us/step - loss: 1.1983 - acc: 0.7402\n",
      "Epoch 84/100\n",
      "6941/6941 [==============================] - 2s 347us/step - loss: 1.1681 - acc: 0.7495\n",
      "Epoch 85/100\n",
      "6941/6941 [==============================] - 2s 341us/step - loss: 1.1498 - acc: 0.7495\n",
      "Epoch 86/100\n",
      "6941/6941 [==============================] - 2s 353us/step - loss: 1.1204 - acc: 0.7594\n",
      "Epoch 87/100\n",
      "6941/6941 [==============================] - 2s 356us/step - loss: 1.0981 - acc: 0.7653\n",
      "Epoch 88/100\n",
      "6941/6941 [==============================] - 2s 339us/step - loss: 1.0680 - acc: 0.7734\n",
      "Epoch 89/100\n",
      "6941/6941 [==============================] - 2s 343us/step - loss: 1.0540 - acc: 0.7693\n",
      "Epoch 90/100\n",
      "6941/6941 [==============================] - 2s 340us/step - loss: 1.0306 - acc: 0.7777\n",
      "Epoch 91/100\n",
      "6941/6941 [==============================] - 2s 328us/step - loss: 1.0000 - acc: 0.7866\n",
      "Epoch 92/100\n",
      "6941/6941 [==============================] - 2s 325us/step - loss: 0.9887 - acc: 0.7866\n",
      "Epoch 93/100\n",
      "6941/6941 [==============================] - 2s 323us/step - loss: 0.9622 - acc: 0.7863\n",
      "Epoch 94/100\n",
      "6941/6941 [==============================] - 2s 336us/step - loss: 0.9458 - acc: 0.7963\n",
      "Epoch 95/100\n",
      "6941/6941 [==============================] - 2s 326us/step - loss: 0.9194 - acc: 0.8015\n",
      "Epoch 96/100\n",
      "6941/6941 [==============================] - 2s 327us/step - loss: 0.9001 - acc: 0.8045\n",
      "Epoch 97/100\n",
      "6941/6941 [==============================] - 2s 330us/step - loss: 0.8830 - acc: 0.8095\n",
      "Epoch 98/100\n",
      "6941/6941 [==============================] - 2s 343us/step - loss: 0.8659 - acc: 0.8090\n",
      "Epoch 99/100\n",
      "6941/6941 [==============================] - 2s 341us/step - loss: 0.8378 - acc: 0.8169\n",
      "Epoch 100/100\n",
      "6941/6941 [==============================] - 2s 336us/step - loss: 0.8284 - acc: 0.8192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f036f43da90>"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.fit(predictors_4, labels_4, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tPoJNaBJRPR"
   },
   "source": [
    "You can see that 4-gram data is very fast to train, each epoch just take 2-3 seconds where as in n-gram data each epoch took around 90 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gm_brrqR92sy"
   },
   "outputs": [],
   "source": [
    "def generate_text(input_text, next_words_count, model, max_len_sequence):\n",
    "    for i in range(next_words_count):\n",
    "        token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        token_list = np.array(pad_sequences([token_list], maxlen=max_len_sequence-1, padding='pre'))\n",
    "        \n",
    "        predicted = model.predict_classes(token_list)\n",
    "        output_word = ''\n",
    "        \n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "                \n",
    "        input_text = input_text + \" \" + output_word\n",
    "        \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdKhlVplJsQ8"
   },
   "source": [
    "Let's compare output of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Np3m0Giy92s1",
    "outputId": "19593761-13b1-44b1-ab80-1aa6f17c1c9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is closely related to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy to the us according to the cost and 223 the first function \n",
      "\n",
      "Deep learning is closely related to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy the cap image is an thousands of transformations 1 steps for number\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"Deep learning is\", 30, model, max_len_sequence),'\\n')\n",
    "\n",
    "print(generate_text(\"Deep learning is\", 30, model_4, max_len_sequence_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "hQSqhCIy92s5",
    "outputId": "473952df-b5ce-40c7-e0c9-43bbfd4837b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is the team led to george intelligent machines in a lack of understanding of the same brain in the us according to be training to produce function player 195 196 197 google \n",
      "\n",
      "It is the universal interpretation and abstraction the done that them an defense is learning used to flow is the game 2 221 training uniform regularization model matching variables and “bad hinton amazon\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"It is the\", 30, model, max_len_sequence),'\\n')\n",
    "\n",
    "print(generate_text(\"It is the\", 30, model_4, max_len_sequence_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "GyyUg69d92s9",
    "outputId": "3669ae19-2465-4fed-cac6-3f0636b87e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial neural networks anns were inspired to produce molecules in the early 2000s when of finite size to approximate time and example the most mathematical nsa and darpa sri deep neural network and \n",
      "\n",
      "Artificial neural networks anns were used 2 such for computer weights and won the ann that adjust the style of data neurons is trained may layer at a reverse treating each the biological\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"Artificial neural networks\", 30, model, max_len_sequence),'\\n')\n",
    "\n",
    "print(generate_text(\"Artificial neural networks\", 30, model_4, max_len_sequence_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "rYuyV-vk92tA",
    "outputId": "42f3922c-8f24-4f33-be24-428dd8b697e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A deep neural network and began and the early 2000s when of finite same into a be representation of deep learning methods in the width and transformed the reverse mathematical b of deep learning \n",
      "\n",
      "A deep neural network with relu activation is strictly larger than the input dimension then deep neural network in speech for a visual verifier boolean the cap and uses which where have been explored\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"A deep neural network\", 30, model, max_len_sequence),'\\n')\n",
    "\n",
    "print(generate_text(\"A deep neural network\", 30, model_4, max_len_sequence_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "wu1OOKdQ92tD",
    "outputId": "83df982f-56c9-4730-d256-0777314bf1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation systems have been used to implementing language models since the early 2000s 109 137 lstm helped of the raw hidden input and the us according to yann lecun 73 not before \n",
      "\n",
      "Recommendation systems have used deep learning to extract meaningful features for the picture factor at user interface to the genetic breed and training for they cases is the ebola virus 158 and\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"Recommendation systems\",30, model, max_len_sequence),'\\n')\n",
    "\n",
    "print(generate_text(\"Recommendation systems\", 30, model_4, max_len_sequence_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Qn_wilZI92tH",
    "outputId": "c97777bd-7e2a-4463-96eb-c165e0d98989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer vision demonstrated in the early 2000s when of finite same is attackers and defenders in the first states d and possibly the first hierarchy to the cost function 223 224 or \n",
      "\n",
      "computer vision be speech recognition tasks to 2011 dramatically while the numbers of the optimization extraction relationship deep networks to extract meaningful features for the picture factor at user interface to the\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"computer vision\",30,model,max_len_sequence),'\\n')\n",
    "\n",
    "print(generate_text(\"computer vision\", 30, model_4, max_len_sequence_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wlEn5SYMZ_F"
   },
   "source": [
    "You can see that both model gave good performance with n_gram relatively high but 4_gram model is very fast to train so you can train 4_gram model for 500 epochs for better performance<br>\n",
    "\n",
    "<b>NOTE:</b></br>\n",
    "4_gram model predict next word based on last four words, But sometimes for predicting word, we need more information then just past four words. What appears before four words may influence next word which we want to predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luLmfy2FP_Ib"
   },
   "source": [
    "Some Other Application Of Language Modelling:\n",
    "<ul><li>Speech recognition</li>\n",
    "<li>Machine Translation</li>\n",
    "<li>Spell Correction</li>\n",
    "<li>Providing Suggestion</li></ul>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Description-Generation_from_Wikipedia.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
